"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[207],{1377(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"modules/06-human-robot-interaction","title":"Module 6: Human-Robot Interaction (HRI)","description":"This module transitions from the mechanics of robot motion to the subtleties of robot interaction. We will explore how to design humanoid robots that can work with, understand, and be understood by humans. This involves cognitive robotics, interaction design, and the emerging field of \\"Emotion AI\\" to make robots more intuitive and socially aware partners.","source":"@site/docs/modules/06-human-robot-interaction.md","sourceDirName":"modules","slug":"/modules/06-human-robot-interaction","permalink":"/ai-book/modules/06-human-robot-interaction","draft":false,"unlisted":false,"editUrl":"https://github.com/tanzeelnaveed8/ai-book/tree/main/docs/modules/06-human-robot-interaction.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"06-human-robot-interaction","title":"Module 6: Human-Robot Interaction (HRI)","sidebar_label":"Human-Robot Interaction"},"sidebar":"tutorialSidebar","previous":{"title":"Bipedal Locomotion","permalink":"/ai-book/modules/05-bipedal-locomotion"},"next":{"title":"Ethics & Society","permalink":"/ai-book/modules/07-ethics-and-society"}}');var o=i(4848),s=i(8453),a=i(7293);const r={id:"06-human-robot-interaction",title:"Module 6: Human-Robot Interaction (HRI)",sidebar_label:"Human-Robot Interaction"},l="Module 6: Human-Robot Interaction (HRI)",c={},d=[{value:"6.1 The Goal of HRI: From Tools to Teammates",id:"61-the-goal-of-hri-from-tools-to-teammates",level:2},{value:"6.2 Cognitive Robotics: Giving the Robot a &quot;Theory of Mind&quot;",id:"62-cognitive-robotics-giving-the-robot-a-theory-of-mind",level:2},{value:"6.3 Interaction Design for Robots",id:"63-interaction-design-for-robots",level:2},{value:"Implicit vs. Explicit Communication",id:"implicit-vs-explicit-communication",level:3},{value:"Proxemics: The Social Use of Space",id:"proxemics-the-social-use-of-space",level:3},{value:"6.4 Emotion AI: Recognizing and Expressing Affect",id:"64-emotion-ai-recognizing-and-expressing-affect",level:2},{value:"Recognizing Human Emotion",id:"recognizing-human-emotion",level:3},{value:"Expressing Emotion",id:"expressing-emotion",level:3},{value:"Assignment Tasks",id:"assignment-tasks",level:3}];function h(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-6-human-robot-interaction-hri",children:"Module 6: Human-Robot Interaction (HRI)"})}),"\n",(0,o.jsx)(a.A,{type:"info",title:"Module Focus",children:(0,o.jsx)("p",{children:'This module transitions from the mechanics of robot motion to the subtleties of robot interaction. We will explore how to design humanoid robots that can work with, understand, and be understood by humans. This involves cognitive robotics, interaction design, and the emerging field of "Emotion AI" to make robots more intuitive and socially aware partners.'})}),"\n",(0,o.jsx)(n.h2,{id:"61-the-goal-of-hri-from-tools-to-teammates",children:"6.1 The Goal of HRI: From Tools to Teammates"}),"\n",(0,o.jsxs)(n.p,{children:["Historically, robots have been treated as tools\u2014caged off in factories, performing repetitive tasks. The goal of modern HRI is to transform robots into ",(0,o.jsx)(n.strong,{children:"teammates"})," that can operate safely and effectively in human-centric environments like homes, hospitals, and offices."]}),"\n",(0,o.jsx)(n.p,{children:"This requires a shift in design philosophy. A successful interactive robot is not just one that performs its task correctly, but one that is also:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Legible:"})," Its actions and intentions are clear to the humans around it."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Predictable:"})," Its behavior is consistent and understandable."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Socially Aware:"})," It adheres to social norms and conventions (e.g., respecting personal space)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Intuitive:"})," Humans can easily learn how to interact with it without extensive training."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"62-cognitive-robotics-giving-the-robot-a-theory-of-mind",children:'6.2 Cognitive Robotics: Giving the Robot a "Theory of Mind"'}),"\n",(0,o.jsxs)(n.p,{children:["A key aspect of HRI is ",(0,o.jsx)(n.strong,{children:"Cognitive Robotics"}),', which aims to endow robots with a limited "Theory of Mind"\u2014the ability to reason about the beliefs, intentions, and knowledge of others. A robot with a cognitive model of its human partner can make much more intelligent decisions.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Example:"}),'\r\nImagine you ask a robot, "Bring me the cup."']}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["A ",(0,o.jsx)(n.strong,{children:"non-cognitive robot"})," might see three cups and stop, confused, or bring the closest one."]}),"\n",(0,o.jsxs)(n.li,{children:["A ",(0,o.jsx)(n.strong,{children:"cognitive robot"}),' might reason: "The human is pointing towards the table. They were just drinking coffee. Therefore, they probably mean the coffee cup on the table."']}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["This is achieved by building a ",(0,o.jsx)(n.strong,{children:"knowledge base"})," and an ",(0,o.jsx)(n.strong,{children:"inference engine"}),". The robot maintains a model of the world and uses logical rules to infer the human's intent based on language, gestures, and context."]}),"\n",(0,o.jsx)(n.h2,{id:"63-interaction-design-for-robots",children:"6.3 Interaction Design for Robots"}),"\n",(0,o.jsx)(n.p,{children:"Interaction Design for HRI involves designing the robot's behaviors and communication modalities. The goal is to make the interaction feel natural and seamless."}),"\n",(0,o.jsx)(n.h3,{id:"implicit-vs-explicit-communication",children:"Implicit vs. Explicit Communication"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Explicit Communication"}),' is direct and unambiguous, such as spoken commands ("Stop") or pressing a button on a GUI.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Implicit Communication"})," is indirect and conveyed through the robot's body language. For example, if a humanoid robot turns its head and eyes toward an object before reaching for it, this telegraphs its intention to the human, making the action feel more predictable and less startling."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Effective HRI design uses a combination of both. A robot might use its gaze to indicate what it's paying attention to (implicit) while listening for a voice command to take action (explicit)."}),"\n",(0,o.jsx)(n.h3,{id:"proxemics-the-social-use-of-space",children:"Proxemics: The Social Use of Space"}),"\n",(0,o.jsx)(n.p,{children:"Proxemics is the study of how people use space. A robot that is socially aware must respect these unwritten rules. For example, it should not stand uncomfortably close to a person unless it is invited to do so. The robot's navigation system should be aware of social zones (intimate, personal, social) and adjust its pathing to maintain an appropriate distance."}),"\n",(0,o.jsx)(n.h2,{id:"64-emotion-ai-recognizing-and-expressing-affect",children:"6.4 Emotion AI: Recognizing and Expressing Affect"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Emotion AI"}),", or Affective Computing, is a field that aims to develop systems that can recognize, interpret, and simulate human emotions. For a social robot, this is a critical capability for building rapport and trust."]}),"\n",(0,o.jsx)(n.h3,{id:"recognizing-human-emotion",children:"Recognizing Human Emotion"}),"\n",(0,o.jsx)(n.p,{children:"A robot can use multiple modalities to infer a person's emotional state:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Facial Expressions:"})," Using computer vision to classify expressions (e.g., happy, sad, surprised)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vocal Tone (Prosody):"})," Analyzing the pitch, volume, and tempo of speech."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Physiology:"})," In some settings, a robot might have access to data from wearable sensors (e.g., heart rate) to gauge arousal or stress."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"expressing-emotion",children:"Expressing Emotion"}),"\n",(0,o.jsx)(n.p,{children:"A humanoid robot can use its own body to express a simplified emotional state to provide feedback to the user."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Body Posture:"})," A slumped posture can indicate failure or sadness, while an upright posture can indicate confidence."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Head Position:"})," A tilted head can signify curiosity or confusion."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simple Facial Displays:"})," Using LED eyes or a simple screen to show expressions can make the robot's internal state much clearer to a human."]}),"\n"]}),"\n",(0,o.jsx)(a.A,{type:"danger",title:"The Ethical Trap of Emotion AI",children:(0,o.jsx)("p",{children:'Emotion AI is ethically complex. There are serious concerns about privacy, the potential for manipulation, and the accuracy and biases of emotion recognition models. This technology must be used transparently and with great care, focusing on improving the clarity of interaction rather than trying to "read minds."'})}),"\n",(0,o.jsx)(n.h3,{id:"assignment-tasks",children:"Assignment Tasks"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Gaze Interaction:"}),' In a simulator, program a robot\'s head to track a moving object or the "user\'s" camera position. Observe how this simple behavior makes the robot seem more "alive" and attentive.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Social Navigation:"}),' Modify a ROS 2 navigation costmap to include a "personal space" layer around a simulated person. The robot should plan paths that avoid entering this zone unless the goal is inside it.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Interaction Design Document:"})," For the capstone project, write a short document describing how the robot will communicate its state to the user. What does it do when it's listening? When it's planning? When it has failed? Use both implicit (body language) and explicit (sound or light) signals."]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},7293(e,n,i){i.d(n,{A:()=>M});var t=i(6540),o=i(4848);function s(e){const{mdxAdmonitionTitle:n,rest:i}=function(e){const n=t.Children.toArray(e),i=n.find(e=>t.isValidElement(e)&&"mdxAdmonitionTitle"===e.type),s=n.filter(e=>e!==i),a=i?.props.children;return{mdxAdmonitionTitle:a,rest:s.length>0?(0,o.jsx)(o.Fragment,{children:s}):null}}(e.children),s=e.title??n;return{...e,...s&&{title:s},children:i}}var a=i(4164),r=i(1312),l=i(7559);const c="admonition_xJq3",d="admonitionHeading_Gvgb",h="admonitionIcon_Rf37",m="admonitionContent_BuS1";function u({type:e,className:n,children:i}){return(0,o.jsx)("div",{className:(0,a.A)(l.G.common.admonition,l.G.common.admonitionType(e),c,n),children:i})}function g({icon:e,title:n}){return(0,o.jsxs)("div",{className:d,children:[(0,o.jsx)("span",{className:h,children:e}),n]})}function p({children:e}){return e?(0,o.jsx)("div",{className:m,children:e}):null}function f(e){const{type:n,icon:i,title:t,children:s,className:a}=e;return(0,o.jsxs)(u,{type:n,className:a,children:[t||i?(0,o.jsx)(g,{title:t,icon:i}):null,(0,o.jsx)(p,{children:s})]})}function x(e){return(0,o.jsx)("svg",{viewBox:"0 0 14 16",...e,children:(0,o.jsx)("path",{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})})}const b={icon:(0,o.jsx)(x,{}),title:(0,o.jsx)(r.A,{id:"theme.admonition.note",description:"The default label used for the Note admonition (:::note)",children:"note"})};function j(e){return(0,o.jsx)(f,{...b,...e,className:(0,a.A)("alert alert--secondary",e.className),children:e.children})}function v(e){return(0,o.jsx)("svg",{viewBox:"0 0 12 16",...e,children:(0,o.jsx)("path",{fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"})})}const y={icon:(0,o.jsx)(v,{}),title:(0,o.jsx)(r.A,{id:"theme.admonition.tip",description:"The default label used for the Tip admonition (:::tip)",children:"tip"})};function w(e){return(0,o.jsx)(f,{...y,...e,className:(0,a.A)("alert alert--success",e.className),children:e.children})}function A(e){return(0,o.jsx)("svg",{viewBox:"0 0 14 16",...e,children:(0,o.jsx)("path",{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"})})}const T={icon:(0,o.jsx)(A,{}),title:(0,o.jsx)(r.A,{id:"theme.admonition.info",description:"The default label used for the Info admonition (:::info)",children:"info"})};function I(e){return(0,o.jsx)(f,{...T,...e,className:(0,a.A)("alert alert--info",e.className),children:e.children})}function k(e){return(0,o.jsx)("svg",{viewBox:"0 0 16 16",...e,children:(0,o.jsx)("path",{fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"})})}const R={icon:(0,o.jsx)(k,{}),title:(0,o.jsx)(r.A,{id:"theme.admonition.warning",description:"The default label used for the Warning admonition (:::warning)",children:"warning"})};function H(e){return(0,o.jsx)("svg",{viewBox:"0 0 12 16",...e,children:(0,o.jsx)("path",{fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"})})}const z={icon:(0,o.jsx)(H,{}),title:(0,o.jsx)(r.A,{id:"theme.admonition.danger",description:"The default label used for the Danger admonition (:::danger)",children:"danger"})};const E={icon:(0,o.jsx)(k,{}),title:(0,o.jsx)(r.A,{id:"theme.admonition.caution",description:"The default label used for the Caution admonition (:::caution)",children:"caution"})};const N={...{note:j,tip:w,info:I,warning:function(e){return(0,o.jsx)(f,{...R,...e,className:(0,a.A)("alert alert--warning",e.className),children:e.children})},danger:function(e){return(0,o.jsx)(f,{...z,...e,className:(0,a.A)("alert alert--danger",e.className),children:e.children})}},...{secondary:e=>(0,o.jsx)(j,{title:"secondary",...e}),important:e=>(0,o.jsx)(I,{title:"important",...e}),success:e=>(0,o.jsx)(w,{title:"success",...e}),caution:function(e){return(0,o.jsx)(f,{...E,...e,className:(0,a.A)("alert alert--warning",e.className),children:e.children})}}};function M(e){const n=s(e),i=(t=n.type,N[t]||(console.warn(`No admonition component found for admonition type "${t}". Using Info as fallback.`),N.info));var t;return(0,o.jsx)(i,{...n})}},8453(e,n,i){i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);