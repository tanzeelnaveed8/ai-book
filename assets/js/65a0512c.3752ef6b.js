"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[416],{7293:(e,n,t)=>{t.d(n,{A:()=>R});var i=t(6540),o=t(4848);function s(e){const{mdxAdmonitionTitle:n,rest:t}=function(e){const n=i.Children.toArray(e),t=n.find(e=>i.isValidElement(e)&&"mdxAdmonitionTitle"===e.type),s=n.filter(e=>e!==t),a=t?.props.children;return{mdxAdmonitionTitle:a,rest:s.length>0?(0,o.jsx)(o.Fragment,{children:s}):null}}(e.children),s=e.title??n;return{...e,...s&&{title:s},children:t}}var a=t(4164),r=t(1312),c=t(7559);const l="admonition_xJq3",d="admonitionHeading_Gvgb",h="admonitionIcon_Rf37",u="admonitionContent_BuS1";function m({type:e,className:n,children:t}){return(0,o.jsx)("div",{className:(0,a.A)(c.G.common.admonition,c.G.common.admonitionType(e),l,n),children:t})}function p({icon:e,title:n}){return(0,o.jsxs)("div",{className:d,children:[(0,o.jsx)("span",{className:h,children:e}),n]})}function g({children:e}){return e?(0,o.jsx)("div",{className:u,children:e}):null}function x(e){const{type:n,icon:t,title:i,children:s,className:a}=e;return(0,o.jsxs)(m,{type:n,className:a,children:[(0,o.jsx)(p,{title:i,icon:t}),(0,o.jsx)(g,{children:s})]})}function f(e){return(0,o.jsx)("svg",{viewBox:"0 0 14 16",...e,children:(0,o.jsx)("path",{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})})}const j={icon:(0,o.jsx)(f,{}),title:(0,o.jsx)(r.A,{id:"theme.admonition.note",description:"The default label used for the Note admonition (:::note)",children:"note"})};function v(e){return(0,o.jsx)(x,{...j,...e,className:(0,a.A)("alert alert--secondary",e.className),children:e.children})}function b(e){return(0,o.jsx)("svg",{viewBox:"0 0 12 16",...e,children:(0,o.jsx)("path",{fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"})})}const A={icon:(0,o.jsx)(b,{}),title:(0,o.jsx)(r.A,{id:"theme.admonition.tip",description:"The default label used for the Tip admonition (:::tip)",children:"tip"})};function L(e){return(0,o.jsx)(x,{...A,...e,className:(0,a.A)("alert alert--success",e.className),children:e.children})}function y(e){return(0,o.jsx)("svg",{viewBox:"0 0 14 16",...e,children:(0,o.jsx)("path",{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"})})}const w={icon:(0,o.jsx)(y,{}),title:(0,o.jsx)(r.A,{id:"theme.admonition.info",description:"The default label used for the Info admonition (:::info)",children:"info"})};function T(e){return(0,o.jsx)(x,{...w,...e,className:(0,a.A)("alert alert--info",e.className),children:e.children})}function k(e){return(0,o.jsx)("svg",{viewBox:"0 0 16 16",...e,children:(0,o.jsx)("path",{fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"})})}const M={icon:(0,o.jsx)(k,{}),title:(0,o.jsx)(r.A,{id:"theme.admonition.warning",description:"The default label used for the Warning admonition (:::warning)",children:"warning"})};function V(e){return(0,o.jsx)("svg",{viewBox:"0 0 12 16",...e,children:(0,o.jsx)("path",{fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"})})}const N={icon:(0,o.jsx)(V,{}),title:(0,o.jsx)(r.A,{id:"theme.admonition.danger",description:"The default label used for the Danger admonition (:::danger)",children:"danger"})};const C={icon:(0,o.jsx)(k,{}),title:(0,o.jsx)(r.A,{id:"theme.admonition.caution",description:"The default label used for the Caution admonition (:::caution)",children:"caution"})};const P={...{note:v,tip:L,info:T,warning:function(e){return(0,o.jsx)(x,{...M,...e,className:(0,a.A)("alert alert--warning",e.className),children:e.children})},danger:function(e){return(0,o.jsx)(x,{...N,...e,className:(0,a.A)("alert alert--danger",e.className),children:e.children})}},...{secondary:e=>(0,o.jsx)(v,{title:"secondary",...e}),important:e=>(0,o.jsx)(T,{title:"important",...e}),success:e=>(0,o.jsx)(L,{title:"success",...e}),caution:function(e){return(0,o.jsx)(x,{...C,...e,className:(0,a.A)("alert alert--warning",e.className),children:e.children})}}};function R(e){const n=s(e),t=(i=n.type,P[i]||(console.warn(`No admonition component found for admonition type "${i}". Using Info as fallback.`),P.info));var i;return(0,o.jsx)(t,{...n})}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}},9269:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>c,toc:()=>d});var i=t(4848),o=t(8453),s=t(7293);const a={id:"04-vla-systems",title:"Module 4: Vision-Language-Action (VLA)",sidebar_label:"Vision-Language-Action"},r="Module 4: Vision-Language-Action (VLA)",c={id:"modules/04-vla-systems",title:"Module 4: Vision-Language-Action (VLA)",description:'This module explores the exciting convergence of Large Language Models (LLMs), computer vision, and robotics. We will learn how to build Vision-Language-Action (VLA) systems that allow a robot to understand natural language commands, perceive the world through vision, and execute complex, multi-step tasks. This is how we give the robot a "cognitive" understanding of its mission.',source:"@site/docs/modules/04-vla-systems.md",sourceDirName:"modules",slug:"/modules/04-vla-systems",permalink:"/ai-book/modules/04-vla-systems",draft:!1,unlisted:!1,editUrl:"https://github.com/your-github-username/ai-book/tree/main/docs/modules/04-vla-systems.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{id:"04-vla-systems",title:"Module 4: Vision-Language-Action (VLA)",sidebar_label:"Vision-Language-Action"},sidebar:"tutorialSidebar",previous:{title:"NVIDIA Isaac",permalink:"/ai-book/modules/03-nvidia-isaac"},next:{title:"Bipedal Locomotion",permalink:"/ai-book/modules/05-bipedal-locomotion"}},l={},d=[{value:"4.1 The VLA Triad: Perception, Cognition, and Action",id:"41-the-vla-triad-perception-cognition-and-action",level:2},{value:"4.2 Voice-to-Action: Using OpenAI Whisper",id:"42-voice-to-action-using-openai-whisper",level:2},{value:"4.3 Cognitive Planning with LLMs",id:"43-cognitive-planning-with-llms",level:2},{value:"4.4 From Plan to Execution: The Action Dispatcher",id:"44-from-plan-to-execution-the-action-dispatcher",level:2},{value:"Ethics and Safety",id:"ethics-and-safety",level:3},{value:"Assignment Tasks",id:"assignment-tasks",level:3}];function h(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"}),"\n",(0,i.jsx)(s.A,{type:"info",title:"Module Focus",children:(0,i.jsx)("p",{children:'This module explores the exciting convergence of Large Language Models (LLMs), computer vision, and robotics. We will learn how to build Vision-Language-Action (VLA) systems that allow a robot to understand natural language commands, perceive the world through vision, and execute complex, multi-step tasks. This is how we give the robot a "cognitive" understanding of its mission.'})}),"\n",(0,i.jsx)(n.h2,{id:"41-the-vla-triad-perception-cognition-and-action",children:"4.1 The VLA Triad: Perception, Cognition, and Action"}),"\n",(0,i.jsx)(n.p,{children:"A VLA system is a pipeline that connects three distinct capabilities:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision (Perception):"}),' The robot "sees" the world. This involves processing raw sensor data (like camera images) to identify objects, understand their state, and build a representation of the scene.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Language (Cognition):"}),' The robot "thinks" about the world. This is where an LLM comes in. It takes a high-level command (e.g., "get me the red apple from the table") and the scene representation from the vision system, and breaks it down into a logical sequence of actions.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action (Execution):"}),' The robot "acts" in the world. The action sequence from the LLM is translated into concrete commands for the robot\'s controllers (e.g., ROS 2 actions or services) to execute.']}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This architecture allows for unprecedented flexibility. Instead of programming a robot for a single task, you give it a generalized ability to reason and plan, enabling it to handle a wide variety of commands in novel situations."}),"\n",(0,i.jsx)(n.h2,{id:"42-voice-to-action-using-openai-whisper",children:"4.2 Voice-to-Action: Using OpenAI Whisper"}),"\n",(0,i.jsxs)(n.p,{children:["The first step in many VLA pipelines is understanding a user's spoken command. ",(0,i.jsx)(n.strong,{children:"OpenAI's Whisper"})," is a state-of-the-art automatic speech recognition (ASR) model that can transcribe spoken language into text with very high accuracy."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"The Pipeline:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio Capture:"})," A ROS node running on the robot (or a connected computer) captures audio from a microphone array."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Transcription:"})," The audio data is sent to the Whisper model (either running locally on a powerful GPU or via an API call)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Text Output:"})," Whisper returns the transcribed text."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Publish Command:"})," The ROS node then publishes this text string to a topic, such as ",(0,i.jsx)(n.code,{children:"/natural_language_command"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This decouples the audio processing from the robot's main logic. Any node in the ROS system can now subscribe to this topic to receive transcribed user commands."}),"\n",(0,i.jsx)(n.h2,{id:"43-cognitive-planning-with-llms",children:"4.3 Cognitive Planning with LLMs"}),"\n",(0,i.jsx)(n.p,{children:'This is the core of the VLA system. An LLM acts as a high-level "task planner." It bridges the semantic gap between a vague human command and the precise, low-level actions a robot can perform.'}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"The Process:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Input Prompt:"}),' A dedicated "planner" node constructs a detailed prompt for the LLM. This prompt is crucial and typically includes:']}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"The User's Command:"}),' The text from the Whisper node (e.g., "Please get me the soda from the fridge").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"A World Representation:"}),' Information from the vision system about the current state of the world (e.g., "Objects detected: ',(0,i.jsx)(n.code,{children:"fridge"})," at [x,y,z], ",(0,i.jsx)(n.code,{children:"table"})," at [a,b,c]. Robot state: ",(0,i.jsx)(n.code,{children:"hand_is_empty"}),'").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"A List of Available Actions:"})," A description of the robot's capabilities in plain English (e.g., \"",(0,i.jsx)(n.code,{children:"move_to(object)"}),": moves the robot in front of an object. ",(0,i.jsx)(n.code,{children:"pick_up(object)"}),": picks up a specified object. ",(0,i.jsx)(n.code,{children:"open(object)"}),': opens an object like a door or fridge.").']}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"LLM Inference:"})," The prompt is sent to an LLM (like GPT-4). The LLM's task is to generate a sequence of the available actions that will accomplish the user's command."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Output Plan:"})," The LLM returns a structured plan, often in a format like JSON:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'[\r\n  {"action": "move_to", "object": "fridge"},\r\n  {"action": "open", "object": "fridge"},\r\n  {"action": "pick_up", "object": "soda"},\r\n  {"action": "move_to", "object": "user"}\r\n]\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"44-from-plan-to-execution-the-action-dispatcher",children:"4.4 From Plan to Execution: The Action Dispatcher"}),"\n",(0,i.jsx)(n.p,{children:'The final piece is a node that translates the LLM\'s plan into actual robot behavior. This "action dispatcher" node:'}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Subscribes"})," to the plan generated by the planner node."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Iterates"})," through the sequence of actions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Calls"})," the corresponding ROS 2 services or action servers for each step. For example, for the first step ",(0,i.jsx)(n.code,{children:'{"action": "move_to", "object": "fridge"}'}),", it would call the ",(0,i.jsx)(n.code,{children:"/navigate_to_pose"})," action server provided by the Nav2 stack, giving it the coordinates of the fridge."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Waits"})," for each action to complete successfully before dispatching the next one."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Handles Errors:"})," If an action fails (e.g., the robot can't find a path to the fridge), the dispatcher can either stop, ask the user for help, or even feed the error back to the LLM to generate a new plan."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"ethics-and-safety",children:"Ethics and Safety"}),"\n",(0,i.jsxs)(n.p,{children:["VLA systems are incredibly powerful, but they also introduce significant safety challenges. An LLM could misunderstand a command or generate an unsafe plan. Therefore, a critical component of any VLA system is a ",(0,i.jsx)(n.strong,{children:"safety layer"})," that validates the LLM's output before execution. This layer might check if a planned action would cause a collision, move a joint beyond its limits, or violate a predefined safety rule. Never trust the LLM's output without verification."]}),"\n",(0,i.jsx)(n.h3,{id:"assignment-tasks",children:"Assignment Tasks"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Whisper Node:"})," Create a Python node that uses a library like ",(0,i.jsx)(n.code,{children:"sounddevice"})," to capture audio and the ",(0,i.jsx)(n.code,{children:"openai"})," library to transcribe it with Whisper. Publish the resulting text to a ROS 2 topic."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM Planner:"})," Write a Python script (it doesn't have to be a ROS node yet) that takes a hard-coded command, a list of objects, and a list of functions, and uses an LLM to generate a JSON plan."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Dispatcher:"}),' Create a ROS 2 node that reads a hard-coded JSON plan and calls dummy ROS 2 services to "execute" the plan, printing a log message for each action.']}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);