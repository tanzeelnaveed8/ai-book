"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[353],{8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}},8904:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"introduction/chapter1","title":"Introduction to Physical AI","description":"Concept Explanation","source":"@site/docs/introduction/chapter1.md","sourceDirName":"introduction","slug":"/introduction/chapter1","permalink":"/ai-book/introduction/chapter1","draft":false,"unlisted":false,"editUrl":"https://github.com/tanzeelnaveed8/ai-book/tree/main/docs/introduction/chapter1.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"chapter1","title":"Introduction to Physical AI","sidebar_position":1},"sidebar":"tutorialSidebar","next":{"title":"Modules","permalink":"/ai-book/modules"}}');var o=i(4848),s=i(8453);const r={id:"chapter1",title:"Introduction to Physical AI",sidebar_position:1},a="Introduction to Physical AI",c={},l=[{value:"Concept Explanation",id:"concept-explanation",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Diagrams Described in Text",id:"diagrams-described-in-text",level:2},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Example 1: Autonomous Mobile Robot Navigation",id:"example-1-autonomous-mobile-robot-navigation",level:3},{value:"Example 2: Humanoid Robot Object Manipulation",id:"example-2-humanoid-robot-object-manipulation",level:3}];function d(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"introduction-to-physical-ai",children:"Introduction to Physical AI"})}),"\n",(0,o.jsx)(n.h2,{id:"concept-explanation",children:"Concept Explanation"}),"\n",(0,o.jsx)(n.p,{children:"Physical AI refers to the integration of artificial intelligence with physical systems, enabling machines to perceive, reason, and act in the real world. Unlike purely software-based AI, Physical AI focuses on embodied intelligence, where agents (such as robots or autonomous vehicles) interact with their environment through sensors and actuators. This field bridges the gap between theoretical AI concepts and their practical application in physical domains, addressing challenges like real-time decision-making, robustness to noise and uncertainty, and safe human-robot interaction."}),"\n",(0,o.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,o.jsx)(n.p,{children:"Upon completing this chapter, readers will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Define Physical AI and differentiate it from traditional AI."}),"\n",(0,o.jsx)(n.li,{children:"Identify key components of a Physical AI system (sensors, actuators, control systems)."}),"\n",(0,o.jsx)(n.li,{children:"Understand the challenges inherent in deploying AI in physical environments."}),"\n",(0,o.jsx)(n.li,{children:"Appreciate the interdisciplinary nature of Physical AI, drawing from robotics, control theory, and computer science."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"diagrams-described-in-text",children:"Diagrams Described in Text"}),"\n",(0,o.jsxs)(n.p,{children:["Imagine a block diagram illustrating a typical Physical AI system. At the center is the ",(0,o.jsx)(n.strong,{children:"AI Core"}),", representing the decision-making and learning algorithms. This core receives input from ",(0,o.jsx)(n.strong,{children:"Sensors"})," (e.g., cameras, lidar, microphones), which perceive the environment. The AI Core processes this sensory data and generates commands for ",(0,o.jsx)(n.strong,{children:"Actuators"})," (e.g., motors, grippers, manipulators), which then execute physical actions in the ",(0,o.jsx)(n.strong,{children:"Physical Environment"}),". A feedback loop exists where the changes in the Physical Environment are again perceived by the Sensors, completing the cycle of perception-reasoning-action. This interaction is mediated by ",(0,o.jsx)(n.strong,{children:"Control Systems"})," that translate high-level AI commands into precise physical movements."]}),"\n",(0,o.jsx)(n.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,o.jsx)(n.h3,{id:"example-1-autonomous-mobile-robot-navigation",children:"Example 1: Autonomous Mobile Robot Navigation"}),"\n",(0,o.jsx)(n.p,{children:"Consider an autonomous mobile robot tasked with navigating an office environment. The robot uses various sensors:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Lidar"}),": To create a 2D map of its surroundings and detect obstacles."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Camera"}),": For visual recognition of objects (e.g., chairs, tables, people) and signs."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Encoders"}),": To track its own movement and position (odometry)."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"AI Core"})," processes this data to perform tasks such as:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Localization"}),": Determining its current position on the map."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Mapping"}),": Building or updating the map of the environment."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Path Planning"}),": Calculating an optimal, collision-free path to a target destination."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Obstacle Avoidance"}),": Adjusting its path in real-time to avoid dynamic obstacles like moving people."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"Actuators"})," (wheels, steering motors) execute the planned movements. The robot constantly updates its perception and plans, demonstrating continuous interaction with the physical world."]}),"\n",(0,o.jsx)(n.h3,{id:"example-2-humanoid-robot-object-manipulation",children:"Example 2: Humanoid Robot Object Manipulation"}),"\n",(0,o.jsx)(n.p,{children:"A humanoid robot is tasked with picking up a specific object from a table. This involves:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Stereo Cameras"}),": To perceive the 3D position and orientation of objects on the table."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Tactile Sensors"}),": In its gripper to detect contact and pressure during grasping."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"AI Core"})," utilizes:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Recognition"}),": Identifying the target object among others."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Grasping Planning"}),": Determining an appropriate grasp pose based on object geometry and gripper capabilities."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Motion Planning"}),": Generating a smooth, collision-free trajectory for the arm and hand to reach and grasp the object."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Actuators"})," (arm and hand motors) execute the planned motions, with force feedback from tactile sensors enabling adaptive grasping. This showcases how AI allows a physical system to perform complex manipulation tasks requiring fine motor control and environmental understanding."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);